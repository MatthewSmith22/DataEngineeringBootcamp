# Airflow Market Data Pipeline

This project implements an Apache Airflow DAG to download and analyze stock market data for AAPL and TSLA from Yahoo Finance.

## Project Overview

The DAG (`marketvol`) schedules a data pipeline that:
- Downloads intraday stock prices (1-minute intervals) for AAPL and TSLA
- Saves the data to CSV files
- Moves files to an HDFS-like directory structure
- Runs analytical queries on the downloaded data

**Schedule**: Runs daily at 6 PM (18:00), Monday through Friday

## Prerequisites

- Python 3.8+
- Apache Airflow 3.0
- Required Python packages:
  - `yfinance`
  - `pandas`
  - `apache-airflow-providers-standard`

## Installation

### 1. Install Airflow 3.0

```bash
pip install apache-airflow==3.0.0
```

### 2. Install Required Dependencies

```bash
pip install yfinance pandas apache-airflow-providers-standard
```

### 3. Set up the DAG

Copy `marketvol_pipeline.py` to your Airflow DAGs folder (default: `~/airflow/dags/`):

```bash
cp marketvol_pipeline.py ~/airflow/dags/
```

## Running the Project

### Start Airflow

```bash
airflow standalone
```

This will start both the webserver and scheduler. The web UI will be available at `http://localhost:8080`

Default credentials are displayed in the terminal output.

### Verify the DAG is Loaded

```bash
airflow dags list | grep marketvol
```

You should see:
```
marketvol | /path/to/dags/marketvol_pipeline.py | airflow | False | dags-folder | None
```

### Manual Testing (Optional)

To test the DAG without waiting for the scheduled time:

```bash
airflow dags test marketvol 2025-10-26
```

Or trigger it from the Airflow UI by clicking the play button (▶️) next to the DAG name.

## DAG Structure

The pipeline consists of 6 tasks:

- **t0 (init_dir)**: Creates temporary directory for the execution date
- **t1 (download_aapl)**: Downloads AAPL stock data
- **t2 (download_tsla)**: Downloads TSLA stock data
- **t3 (move_aapl)**: Moves AAPL data to HDFS location
- **t4 (move_tsla)**: Moves TSLA data to HDFS location
- **t5 (run_query)**: Runs analysis on both datasets

### Task Dependencies

```
    t0
   /  \
  t1  t2
  |   |
  t3  t4
   \ /
    t5
```

- t1 and t2 run in parallel after t0
- t3 and t4 run after their respective downloads
- t5 runs only after both t3 and t4 complete

## Verifying Results

### Check DAG Runs

In the Airflow UI:
1. Navigate to `http://localhost:8080`
2. Click on the `marketvol` DAG
3. View the Grid to see run history
4. Click on individual task instances to view logs

### Check Downloaded Files

Files are stored in `/tmp/hdfs/{execution_date}/`:

```bash
ls -la /tmp/hdfs/2025-10-26/
```

You should see:
- `AAPL.csv`
- `TSLA.csv`

### View Task Logs

From the command line:

```bash
airflow tasks test marketvol run_query 2025-10-26
```

Or view logs in the UI by clicking on a task instance and selecting "Log".

## Configuration

### Retry Settings

The DAG is configured to:
- Retry failed tasks **2 times**
- Wait **5 minutes** between retries

### Schedule

The DAG runs:
- **Time**: 6:00 PM (18:00) UTC
- **Days**: Monday through Friday
- **Interval**: Once per day

To change the schedule, modify the `schedule` parameter in `marketvol_pipeline.py`:

```python
schedule='0 18 * * 1-5',  # cron expression
```

## Troubleshooting

### DAG Not Appearing

```bash
# Check for import errors
airflow dags list-import-errors

# Verify dependencies are installed
python -c "import yfinance; import pandas; print('OK')"

# Restart Airflow
pkill -f airflow
airflow standalone
```

### No Data Downloaded

The market is closed on weekends and outside trading hours. The DAG will still run but may not retrieve data. Check task logs for details.

### Permission Errors

Ensure the user running Airflow has write permissions to `/tmp/data/` and `/tmp/hdfs/`:

```bash
mkdir -p /tmp/data /tmp/hdfs
chmod 755 /tmp/data /tmp/hdfs
```

## Project Requirements Met

✅ DAG named "marketvol"  
✅ Runs at 6 PM on weekdays  
✅ Downloads AAPL and TSLA data with 1-minute intervals  
✅ Parallel download tasks (t1, t2)  
✅ Moves data to HDFS location (t3, t4)  
✅ Runs query after all downloads complete (t5)  
✅ Retry logic: 2 retries with 5-minute delay  
✅ Uses BashOperator and PythonOperator  

## Running for Multiple Days

To fulfill the project requirement of running for at least 2 days:

1. Start Airflow: `airflow standalone`
2. Let it run for at least 2 weekdays
3. Monitor runs in the UI
4. Scheduler logs are in `~/airflow/logs/scheduler/`

## File Structure

```
.
├── marketvol_pipeline.py    # Main DAG file
├── README.md                # This file
└── logs/                    # Execution logs (generated by Airflow)
```

## Author

Created for Airflow Mini-Project: DAG Scheduling Exercise